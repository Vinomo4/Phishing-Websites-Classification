```{r}
library(naivebayes)
library(MASS)
library(klaR)
library(nnet)
library(caret)
library(randomForest)
library(ggplot2) 
library(kernlab)
library(reshape)
```

*Loading of the data*

The first step of the practice consists of reading the chosen database, its initial visualization as well as carrying out a pre-process, with the aim of eliminating and / or correcting possible anomalies.

```{r}
dd = read.csv2("./Data.csv",sep = ',')
head(dd)
```

*Establishing the variables as categorical*

```{r}
dd[,31] = as.factor(dd[,31])
#Checking that the conversion was successful.
for (i in 1:ncol(dd)){
  cat("The variable",colnames(dd)[i],"is factor:",is.factor(dd[,i]),"\n")
}
```
*Checking missing data*

```{r}
sum(is.na(dd))
```
As we can see, we do not have any missing data encoded as $ NA $. By observing the summary of the data, we can verify that all the values are located within the margins established in the surveys (each variable is evaluated as (-1,1) in the case of the binary variables and (-1 , 0.1) in the case of ternaries).

```{r}
summary(dd)
```
We modify the levels of the variable *Result* to *phishing* i *legit* so that they are more interpretable:

```{r}
levels(dd$Result) = c("Phishing", "Legit")
table(dd$Result)
```
```{r}
par(mfrow=c(2,2))
for(i in 1:31){
  aux = ggplot(dd, aes(x = dd[,i])) + geom_bar(aes(fill = Result),position = "dodge") + labs(title = colnames(dd)[i], x = "Values", y = "Frequencies")
  print(aux)
}
```
```{r}
library(ca)
cont3 = table(dd$Prefix_Suffix, dd$web_traffic, dd$URL_of_Anchor)
mj1 = mjca(cont3, lambda = "indicator")
```



```{r}
mj1 
mj1$inertia.t # total inertia
mj1$nd
```


```{r}
plot(mj1)
points(mj1$rowpcoord)
```
*Kernel PCA*

```{r}
kpc = kpca(as.matrix(dd[1:1000,-31], kernel = "rbfdot", kpar = list(sigma = 1)))
```

```{r}
plot(rotated(kpc),col=as.integer(dd[1:1000,31]),
     xlab="1st Principal Component",ylab="2nd Principal Component")
```
*Splitting data in training and test*

```{r}
set.seed(221220)
N = nrow(dd)
selection = sample(N, round(N*0.67))
Test = dd[-selection,1:31]
Train = dd[selection,1:31]

X_Train = as.matrix(Train[,1:30])
Y_Train = Train[,31]
X_Test = as.matrix(Test[,1:30])
Y_Test = Test[,31]
```

#Error Kernel Mehtods
```{r}
error_kernel = function (X_Learning, Y_Learning, X_Validation, Y_Validation, kernel_type, Flag) {
   
   svm = ksvm(X_Train, Y_Train, kernel=kernel_type, scale = c())
  
   PI = predict(svm,X_Validation)
   prediction = table(Truth = Y_Validation, Pred = PI)
   if (Flag) print(prediction)
   err = 1 - sum(diag(prediction))/length(Y_Validation)
}
```


```{r}
prior = function(X){
  prob_matrix = matrix(nrow =30, ncol = 3)
  for (i in 1:30){
    aux =  prop.table(table(X[,i]))
    prob_matrix[i,1] = aux['-1']
    prob_matrix[i,2] = aux['0']
    prob_matrix[i,3] = aux['1']
  }
  prob_matrix[is.na(prob_matrix)] = 0 
  prob_matrix
}
```


#Error Naive Bayes
```{r}
error_naivebayes = function (X_Learning, Y_Learning, X_Validation, Y_Validation, Flag) {
   # Generació del model amb les dades de learn
   mod = naive_bayes(X_Learning, Y_Learning, laplace = 1)
   
   # Predicció sobre les dades de validació
   PI = predict(mod, newdata = X_Validation, type = "class")
   prediction = table(Truth = Y_Validation, Pred = PI)
   if (Flag) print(prediction)
   err = 1 - sum(diag(prediction))/length(Y_Validation)
}
```


#Error RF
```{r}
error_randomforest = function (X_Learning, Y_Learning, X_Validation, Y_Validation, k, Flag) {
   rf = randomForest (Y_Learning ~ ., data = as.data.frame(X_Learning), ntree = k, proximity = FALSE)
   pred = predict(rf, newdata = X_Validation, type = "class") 
   prediction = table(Truth = Y_Validation, Pred = pred)
   if (Flag) print(prediction)
   err = 1 - sum(diag(prediction))/length(Y_Validation)
}
```

# Auxiliar 5x5 CV Hyperparameters function.
```{r}
#Input: Model to test, list of values for the parameter.

cross_validation = function(model,parameter_list,type,fixed_parameter,title){
set.seed(221220)
N = nrow(X_Train)
size = length(parameter_list)
le.error = rep(0, size)
val.error = rep(0, size)
cont = 1
m = 5
k = 5

for (param in parameter_list) {
   err = c("learning" = 0, "validation" = 0)
   for (i in 1:m) {
      selection = sample(N)  # Randomly mixing training data
      for (j in 1:k) {
         start = round((j - 1)*N/10 + 1)  # Start index
         end = round(j * N/10)  # Final index
         
         # Splitting in learning and validation
         X_Learning = X_Train[selection[-(start:end)],]
         Y_Learning = Y_Train[selection[-(start:end)]]
         X_Validation = X_Train[selection[start:end],]
         Y_Validation = Y_Train[selection[start:end]]
         
         if(type == "RF"){
           error_val = error_randomforest(X_Learning, Y_Learning, X_Validation, Y_Validation, param, F)
           error_learn = error_randomforest(X_Learning, Y_Learning, X_Learning, Y_Learning, param, F)
         }
         # It's a kernel method.
         else{
           if(type =="RBF"){
             param_model = model(sigma = param)
           }
           else{
              P_matrix = prior(X_Learning)
              if(type == "Multivariate"){
                if(fixed_parameter[1] == "alpha"){param_model = model(P_matrix,as.double(fixed_parameter[2]),param)}
                else{param_model = model(P_matrix,param,as.double(fixed_parameter[2]))}
              }
              # GL kernels
              else{param_model = model(P_matrix,param)}
           }
           error_val = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation, param_model, F)
           error_learn = error_kernel(X_Learning, Y_Learning, X_Learning, Y_Learning, param_model, F)
                   
         }
         err["validation"] = err["validation"] + error_val 
         err["learning"] = err["learning"] +  error_learn 
      }
   }
   print(param)
   le.error[cont] = err["learning"]/(m*k)
   val.error[cont] = err["validation"]/(m*k)
   cont = cont + 1
}

plot = ggplot(as.data.frame(le.error), aes(parameter_list)) + 
  geom_line(aes(y = le.error, colour = "le.error")) + 
  geom_line(aes(y = val.error, colour = "val.error")) +
  labs(x = "Parameter", y = "error") +
  ggtitle(title)+  theme(plot.title = element_text(hjust = 0.5))

ggsave(title,plot=plot,path = "./CV_Plots",device = "png")
}
```


```{r}
# Applying CV for the method that have hyperparameters.
    alpha = seq(0.2,2,0.2)
    ntrees = seq(1,101,2)
    cross_validation("",ntrees,"RF",c(),"RF")
    cross_validation(GL_3.kernel,alpha,"GL",c(),"GL3")
    cross_validation(GL_4.kernel,alpha,"GL",c(),"GL4")
    cross_validation(GL_5.kernel,alpha,"GL",c(),"GL5")#
    cross_validation(GL_6.kernel,alpha,"GL",c(),"GL6")
    cross_validation(GL_7.kernel,alpha,"GL",c(),"GL7")
    cross_validation(GL_9.kernel,alpha,"GL",c(),"GL9")
    cross_validation(GL_12.kernel,alpha,"GL",c(),"GL12")
    cross_validation(GL_13.kernel,alpha,"GL",c(),"GL13")
    cross_validation(GL_14.kernel,alpha,"GL",c(),"GL14")
    cross_validation(multivariate.kernel,alpha,"Multivariate",c("gamma",0.5),"MV")
    cross_validation(multivariate_id.kernel,alpha,"Multivariate_id",c(),"MV ID")
    cross_validation(multivariate_center.kernel,alpha,"Multivariate",c("Gamma",0.5),"MV C")
```

GL3 = 0.75 (Non-significant)
GL4 = 1
GL5 = 0.5 (Non-significant)
GL6 = 1.25
GL7 = 1.75
GL9 = 1.25
GL12 = 1.75
GL13 = 1
GL14 = 1.25
MV = 1.75
MV C = 1.75
MV ID = 1.25
RF = 60 

```{r}
gamma = seq(0.1,4,0.3)
cross_validation(multivariate.kernel,gamma,"Multivariate",c("alpha",1.75),"MV Gamma")
cross_validation(multivariate_center.kernel,gamma,"Multivariate",c("alpha",1.75),"MV C Gamma")

```

MV Gamma = 1.9
MV C Gamma = 3.75

```{r}
source('Kernels.R')
```

```{r}
set.seed(221220)
N = nrow(X_Train)
m = 5
k = 5

error_matrix = matrix(0, nrow = k*m, ncol =17)

for (i in 1:m) {
  selection = sample(N)
  for (j in 1:k) {
    start = round((j - 1)*N/k + 1)  # Initial index of the j-th validation test
    end = round(j * N/k)  # Índex final
    # Split in explicative/target variables/learn/validation
    X_Learning = X_Train[selection[-(start:end)],]
    Y_Learning = Y_Train[selection[-(start:end)]]
    X_Validation = X_Train[selection[start:end],]
    Y_Validation = Y_Train[selection[start:end]]
    
    P_matrix = prior(X_Learning)
    
    multivariate = multivariate.kernel(P_matrix,1.9,1.75)
    multivariate_id = multivariate_id.kernel(P_matrix,1.25)
    chulito = multivariate_center.kernel(P_matrix,3.75,1.75)
    rbfkernel = rbfdot(sigma = 1)
    
    GL3 = GL_3.kernel(P_matrix,0.75,F)
    GL4 = GL_4.kernel(P_matrix,1,F)
    GL5 = GL_5.kernel(P_matrix,0.5,F)
    GL6 = GL_6.kernel(P_matrix,1.25,F)  
    GL7 = GL_7.kernel(P_matrix,1.75,F)
    GL9 = GL_9.kernel(P_matrix,1.25,F)
    GL12 = GL_12.kernel(P_matrix,1.75,F)
    GL13 = GL_13.kernel(P_matrix,1,F) 
    GL14 = GL_14.kernel(P_matrix,1.25,F) 
    
    error_matrix[(i-1)*k+j,1] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation, overlap.kernel,F)
    print(1)
    error_matrix[(i-1)*k+j,2] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,SMC.kernel, F)
    print(2)
    error_matrix[(i-1)*k+j,3] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation, multivariate, F)
    print(3)
    error_matrix[(i-1)*k+j,4] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation, multivariate_id, F)
    print(4)
    error_matrix[(i-1)*k+j,5] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation, chulito, F)
    print(5)
    error_matrix[(i-1)*k+j,6] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation, rbfkernel, F)
    print(6)
    error_matrix[(i-1)*k+j,7] = error_naivebayes(X_Learning, Y_Learning, X_Validation, Y_Validation, F)
    print(7)
    error_matrix[(i-1)*k+j,8] = error_randomforest(X_Learning, Y_Learning, X_Validation, Y_Validation, 60, F)
    print(8)
    error_matrix[(i-1)*k+j,9] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL3, F)
    print(9)
    error_matrix[(i-1)*k+j,10] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL4, F)
    print(10)
    error_matrix[(i-1)*k+j,11] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL5, F)
    print(11)
    error_matrix[(i-1)*k+j,12] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL6, F)
    print(12)
    error_matrix[(i-1)*k+j,13] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL7, F)
    print(13)
    error_matrix[(i-1)*k+j,14] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL9, F)
    print(14)
    error_matrix[(i-1)*k+j,15] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL12, F)
    print(15)
    error_matrix[(i-1)*k+j,16] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL13, F)
    print(16)
    error_matrix[(i-1)*k+j,17] = error_kernel(X_Learning, Y_Learning, X_Validation, Y_Validation,GL14, F)
    print(17)
    print("##################################################")
  }
}
```
# Evaluation of the obtained errors.

```{r}
error_matrix
error_df = as.data.frame(error_matrix)
colnames(error_df) =c("OV","SMC","MV","MV ID", "MV C", "RBF", "NB", "RF","GL3","GL4","GL5","GL6","GL7","GL9","GL12","GL13","GL14")
```
# Average error by method

```{r}
colMeans(error_df)*100
```
```{r}
boxplot(error_df,les =2 )
```
# Test of the best models 
RBF, GL13, GL14
```{r}
P_matrix = prior(X_Train)
rbfkernel = rbfdot(sigma = 1)
GL13 = GL_13.kernel(P_matrix,1,F) 
multivariate = multivariate.kernel(P_matrix,1.9,1.75)

svm_rbf = ksvm(X_Train, Y_Train, kernel=rbfkernel, scale = c())
print('rbf done')
svm_gl13 = ksvm(X_Train, Y_Train, kernel=GL13, scale = c())
print('gl13 done')
svm_MV = ksvm(X_Train, Y_Train, kernel=multivariate, scale = c())
print('mv done')
```

```{r}
PI_rbf = predict(svm_rbf,X_Test)
prediction = table(Truth = Y_Test, Pred = PI_rbf)
print(prediction)
err = 1 - sum(diag(prediction))/length(Y_Test)
print(err)
```

```{r}
PI_gl13 = predict(svm_gl13,X_Test)
prediction = table(Truth = Y_Test, Pred = PI_gl13)
print(prediction)
err = 1 - sum(diag(prediction))/length(Y_Test)
print(err)
```

```{r}
PI_MV = predict(svm_MV,X_Test)
prediction = table(Truth = Y_Test, Pred = PI_MV)
print(prediction)
err = 1 - sum(diag(prediction))/length(Y_Test)
print(err)
```


